# writeups                                                                             
This repository contains some thoughts (usually additional geometric transparency) for various machine learning algorithms.
                                                                             
### Regression                                                              
                                                                             
I was teaching myself some classical statistics and was a bit puzzled as to what was meant by "degrees of freedom" (dimension) or how the F distribution arose in F-tests and ANOVA. This writeup is a treatment of these topics with a geometrical flavor using norms, vector spaces, and projection operators rather than matrix manipulations.

### SVM
Rewrote the objective in terms of distance - makes the maximum margin classifier aspect of the SVM a little more clear and makes it obvious which vectors are support vectors. Additionally, derived the property that the weight vector is a linear combination of support vectors using a simple geometric argument and avoiding the use of Lagrange multipliers.

### PCA

Made explicit the geometric connection (orthogonality) between the two PCA formulations 1. maximizing the variance and 2. minimizing the reconstructon error.
